---
description: Template to generate ai
keywords:
    - ai
    - ai scaffold
    - ai boilerplate
    - ai generator
    - chatgpt
    - Large Language Models
    - code gen
sidebar_label: ai
---

import { TemplateOptions } from '@site/docs/components/templateOptions';
import { Example } from '@site/docs/components/example';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import { Button } from '@site/docs/components/button';
import { Badge, BadgeList, VersionBadge } from '@site/docs/components/badges';

# ai

<BadgeList>
    <VersionBadge version="1.0.2" library="tps-ai" />
    <VersionBadge version=">1.2.11" library="templates" />
</BadgeList>

Generate any codebase in any language, setup files for any library, or bring any
idea to lifeâ€”all powered by AI.

:::info

Have ideas for new features or improvements? We'd love to hear from you! Please
share your thoughts by submitting a feature request using the button below.

<Button
    to="https://github.com/marcellino-ornelas/templates/discussions/categories/tps-ai-feature-requests-feedback"
    text="Submit feature request"
    size="sm"
    onClick={() => {
        if (typeof gtag === 'function') {
            gtag('event', 'feedback_request_click', {
                event_category: 'Feedback',
                event_label: 'tps-ai',
            });
        }
    }}
/>

:::

## Usage

```bash title="Usage"
tps ai [app-name]

# or in a directory

tps ai ./path/to/dir/[app-name]
```

```txt title="Creates"
| - [app-name]/
    | <files and directories llm creates>
```

<Example>

<Tabs>

<TabItem value="default">

```bash
tps ai funny-gifs
```

</TabItem>

<TabItem value="long build path">

```bash
tps ai projects/funny-gifs
```

</TabItem>

<TabItem value="no build path">

```bash
tps ai
```

</TabItem>

</Tabs>

</Example>

Check out our [examples section](#examples) to see detailed instructions on how
to use this template.

## Installation

This template is not included in the templates library and must be installed
separately.

```bash
npm i -g templates-mo tps-ai

# Or if you already have the templates library installed
npm i -g tps-ai
```

## Options

<TemplateOptions template="tps-ai" type="js" />

## Configuration

To use this template, you need to know which LLM provider you want to use, the
model you'll use, and have all the required credentials. Each LLM provider
required different credentials, so refer to [AI providers](#ai-providers)
section to learn how to configure that llm provider.

When generating a new instance, you will be prompted for your token, AI
provider, and model. However, we recommend using one of the following methods to
answer these questions:

-   **Global Config File (recommended)**
-   **Env variables**
-   **On the CLI**

<Tabs>

<TabItem value="Global Config (recommended)">

You can define your preferred AI provider, model, and API token in your
[global config file](../tpsrc.mdx#make-a-global-configuration-file) (e.g.,
`~/.tps/.tpsrc`). We recommend adding this to your global config file because
your local config file is more likely to be checked into your codebase,
potentially leaking your credentials. Plus, defining these at a global level
allows you to use these credentials throughout your file system.

Run the following command to initialize your
[global config file](../tpsrc.mdx#make-a-global-configuration-file) if you
haven't already:

```bash
tps init -g
```

Then add your preferred AI provider, model, and API token to `~/.tps/.tpsrc`, as
shown below:

```json title="~/.tps/.tpsrc"
{
    "ai": {
        "answers": {
            "token": "<token>",
            "model": "gpt-4o",
            "llm": "openai"
        }
    }
}
```

</TabItem>

<TabItem value="Env variables">

Each LLM provider supports a unique enviroment varible that you can use for
their credentials. You can refer to the [AI provider](#ai-providers) section for
the provider your using to see what ENV varible you need to use. Its important
to remember that only the provider credentials are supported by ENV variables.
The models and provider will still need to be specified using one of our other
[methods](../generating-instance#using-a-templates-options).

Templates does not load environment variables by default, so you will need to
set these in your
[shell before running the command](https://chlee.co/how-to-setup-environment-variables-for-windows-mac-and-linux/).

:::tip

Templates plans on supporting loading environment variables automatically from a
`.env` file in a future release.

:::

</TabItem>

<TabItem value="CLI">

You can pass in your preferred AI provider, model, and API token when generating
your new instance, as shown below:

```bash
tps ai --llm openai --model gpt-4o --token <token>
```

</TabItem>

</Tabs>

## AI Providers

Templates supports multiple AI providers, each with its own unique setup. Below
are the supported AI providers and how to configure them.

Templates uses Vercel's [AI SDK](https://sdk.vercel.ai/) to interact with the AI
providers. Currently, we only support a subset of the
[providers](https://sdk.vercel.ai/providers/ai-sdk-providers) that the AI SDK
supports. The model you choose needs to support object generation in order to
use it with this template. If you would like to see a provider that we don't
support, drop us a
[request](https://github.com/marcellino-ornelas/templates/discussions/categories/tps-ai-feature-requests-feedback).

:::caution

Templates does not support passing options to the LLM models at the moment.
However, this is a feature that we are looking to support in the future.

:::

### OpenAI

**Supported models:**
https://sdk.vercel.ai/providers/ai-sdk-providers/openai#model-capabilities

**Env variable:** `OPENAI_API_KEY`

**Docs:** https://sdk.vercel.ai/providers/ai-sdk-providers/openai

**API key Guide:**
https://medium.com/@duncanrogoff/how-to-get-your-openai-api-key-in-2024-the-complete-guide-6b52d82c7362

To use OpenAI, you need to have an API key. You can get one by following the
[OpenAI API key guide](https://medium.com/@duncanrogoff/how-to-get-your-openai-api-key-in-2024-the-complete-guide-6b52d82c7362).

Once you have your API key, you can follow the [configuration](#configuration)
section to set it up. This provider supports specifing your API token using the
`OPENAI_API_KEY` ENV variable.

### Anthropic (alpha)

**Supported models:**
https://sdk.vercel.ai/providers/ai-sdk-providers/anthropic#model-capabilities

**Env variable:** `ANTHROPIC_API_KEY`

**Docs:** https://sdk.vercel.ai/providers/ai-sdk-providers/anthropic

**API key docs:**
https://docs.anthropic.com/en/api/getting-started#accessing-the-api

To use Anthropic, you need to have an API key. You can get one by following the
[Anthropic API key docs](https://docs.anthropic.com/en/api/getting-started#accessing-the-api).

Once you have your API key, you can follow the [configuration](#configuration)
section to set it up. This provider supports specifing your API token using the
`ANTHROPIC_API_KEY` ENV variable.

### Amazon Bedrock (alpha)

**Supported models:**
https://sdk.vercel.ai/providers/ai-sdk-providers/amazon-bedrock#model-capabilities

**Env variable:** `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`

**Docs:** https://sdk.vercel.ai/providers/ai-sdk-providers/amazon-bedrock

**API key docs:**
https://sdk.vercel.ai/providers/ai-sdk-providers/amazon-bedrock#authentication

:::danger

This providers require `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`,
`AWS_REGION` env variables.

:::

:::caution

The `token` and `baseUrl` options are not supported by this provider.

:::

To use Amazon Bedrock you will need a access key, secret access key, and the
[Amazon Bedrock authentication docs](https://sdk.vercel.ai/providers/ai-sdk-providers/amazon-bedrock#authentication).

Once you have your credentials, you will need to provide them via ENV variables.
You can provided these env variables by following the env variables tab in
[configuration](#configuration) section. You will need to provide all the
folllowing env variables `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`,
`AWS_REGION`.

### Google (alpha)

**Supported models:**
https://sdk.vercel.ai/providers/ai-sdk-providers/google-generative-ai#model-capabilities

**Env variable:** `GOOGLE_GENERATIVE_AI_API_KEY`

**Docs:** https://sdk.vercel.ai/providers/ai-sdk-providers/google-generative-ai

**API key docs:** https://ai.google.dev/gemini-api/docs/api-key

To use Google, you need to have an API key. You can get one by following the
[Google API key docs](https://ai.google.dev/gemini-api/docs/api-key).

Once you have your API key, you can follow the [configuration](#configuration)
section to set it up. This provider supports specifing your API token using the
`GOOGLE_GENERATIVE_AI_API_KEY` ENV variable.

### Deepseek (alpha)

**Supported models:**
https://sdk.vercel.ai/providers/ai-sdk-providers/deepseek#model-capabilities

**Env variable:** `DEEPSEEK_API_KEY`

**Docs:** https://sdk.vercel.ai/providers/ai-sdk-providers/deepseek

**API key docs:** https://api-docs.deepseek.com/

To use Deepseek, you need to have an API key. You can get one by following the
[Deepseek API key docs](https://api-docs.deepseek.com/).

Once you have your API key, you can follow the [configuration](#configuration)
section to set it up. This provider supports specifing your API token using the
`DEEPSEEK_API_KEY` ENV variable.

### Azure (alpha)

**Env variable:** `AZURE_API_KEY`, `AZURE_RESOURCE_NAME`

**Docs:** https://sdk.vercel.ai/providers/ai-sdk-providers/azure

**API key docs:**
https://docs.mindmac.app/how-to.../add-api-key/create-azure-openai-api-key

:::danger

This providers requires the `AZURE_RESOURCE_NAME` env variable.

:::

:::tip

Use the `model` option to specify azures deployment name.

:::

To use Azure, you need to have an API key. You can get one by following the
[Azure API key docs](https://docs.mindmac.app/how-to.../add-api-key/create-azure-openai-api-key).

Once you have your API key, you can follow the [configuration](#configuration)
section to set it up. This provider supports specifing your API token using the
`AZURE_API_KEY` ENV variable.

You also need to specify a `AZURE_RESOURCE_NAME` ENV variable to use this
provider. This is the name of the auzure resource you want to use. You can
provided these env variables by following the env variables tab in
[configuration](#configuration) section.

## Examples

### How to use

Use our AI template to generate anything your mind desires. If you haven't
already, please follow our [prerequisites](#prerequisites) before using this
template.

Once configured, you're ready to use this template. This template can be used in
two ways:

-   With a build path
-   Without a build path

:::tip Tip: What is a Build Path?

The build path is the name and location you provide for your new instance.
Templates uses it to create a directory and render its contents there.

_example:_

```bash
tps ai food-app
    #  |_______|
    #    ^ Build path and instance name

tps ai ./project/food-app
    #  |________| |______|
    #   ^ location    ^ instance name
    #  |_________________|
    #      ^ build path
```

You can learn more about build paths in the
[build path docs](../generating-instance.mdx#what-is-a-build-path).

:::

#### Build path

When generating a new instance with a build path, Templates will create a new
directory based on this build path and put any files or folders the LLM creates
into this directory.

_Example:_

If you want to build an Express app for your new food app idea named `food-app`,
you can run the following command:

```bash
tps ai food-app
? What would you like to build? Create the contents of an Express app. I want all the bells and whistles. It should have a MongoDB connection as well.
```

This will generate a `food-app` directory in your `CWD` and place all files and
directories the LLM creates inside it.

```txt title="Creates"
| - food-app/
    | - <files and directories created by LLM...>
```

#### No Build Path

When generating a new instance with no build path, all files or folders the LLM
creates will be placed into your `CWD`. This is useful if you want content to be
generated inside your current working directory or when you want the LLM to
generate a directory for you. However, **you need to explicitly tell the LLM to
do this.**

_Example:_

If you have an existing repo and want to add Jest support, placing all unit
tests inside a `__tests__` directory:

```bash
tps ai
? What would you like to build? Create a jest.config.js file that will run my Node library. Its all written in JavaScript, and at least one test file should live in the `__tests__` folder.
```

This will generate a `__tests__` directory and a `jest.config.js` file inside
your `CWD`.

```txt title="Creates"
| - <cwd>/
    | - __tests__/
        | - <test files and directories created by LLM...>
    | - jest.config.js
```

:::tip

The more specific you are with the LLM, the better outcome you will get.

For example:

-   **Vague:** "Create an API."
-   **Specific:** "Create a RESTful API using Express with routes for user
    authentication and a MongoDB connection."

Providing clear details helps the ai understand your exact needs and deliver
more accurate results.

:::

### Base Url

The `baseUrl` option lets you change the base url that this template sends the
request to. This is useful when you want to interact with different
environments. Most AI providers supports this base URL option but always refer
to the [provider's docs](#ai-providers) to make sure.

<Tabs>

<TabItem value="Config">

```json title=".tps/.tpsrc"
{
    "ai": {
        "answers": {
            "baseUrl": "https://my-custom-ai.com/api"
        }
    }
}
```

</TabItem>

<TabItem value="CLI">

```bash
tps ai --baseUrl https://my-custom-ai.com/api
```

</TabItem>

</Tabs>

## Inspiration

Need some inspiration on what you can do? Here are some examples:

-   `tps ai`: Generate a README file for my library. This will be published on
    GitHub, so include common sections and best practices for a Node.js library.

-   `tps ai`: Create a jest.config.js file to run tests for my Node.js library,
    which is currently written in JavaScript. Include at least one test file
    located in the \_\_tests\_\_ folder.

-   `tps ai`: Create a GitHub Action file for my Node.js library. The action,
    named ci, should run npm test using Node.js v18. It should trigger on pull
    requests when I push a commit.

-   `tps ai <app-name>`: Create the contents of a full-featured Express app
    called food-app, with all the bells and whistles. Include a MongoDB
    connection as part of the setup.
